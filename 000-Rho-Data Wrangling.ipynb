{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "agricultural-lightweight",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## GIGO, Feed your algorithm right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-terry",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It makes sense to keep your data in a separate folder from your source code. \n",
    "# It allows you to upload code to Github withou compromizing the data you are working on.\n",
    "# The ../ looks one folder up\n",
    "\n",
    "raw = pd.read_csv(\"../data/churn_wgu/churn_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-phone",
   "metadata": {},
   "source": [
    "# First Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.Tenure.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52 states? (Includes DC and PR.)\n",
    "\n",
    "raw.Job.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean indexing (filtering data)\n",
    "raw[(raw.Job == 'Occupational psychologist') & (raw.Gender == 'Female') & (raw.Marital != ('Married')) & raw.Age.between(18, 31, inclusive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.sort_values(by=['Population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Handling\n",
    "# Series.str\n",
    "# Series.str.contains()\n",
    "# Series.str.startswith()\n",
    "# Series.str.isnumeric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-eugene",
   "metadata": {},
   "source": [
    "# Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a subset of all columns based on a datatype\n",
    "\n",
    "returns = data[[key for key in dict(data.dtypes) if dict(data.dtypes)[key] in ['float64', 'int64']]]\n",
    "\n",
    "# calculating perc_changes\n",
    "returns = returns.pct_change()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-scheme",
   "metadata": {},
   "source": [
    "## Descriptive Statistics (Measuring Central Tendencies & Dispersion)\n",
    "\n",
    "### Summarize important elements in a dataset, to understand what it looks like\n",
    "\n",
    "### Univariate\n",
    "\n",
    "- ### Measures of Central Tendency\n",
    "    - #### mean, median, mode\n",
    "    - #### gemetric & harmonic means (ratios & rates)\n",
    "\n",
    "- ### Measures of Dispersion\n",
    "    - #### range (max - min)\n",
    "    - #### Inter-Quartile Range - middle 50% of data\n",
    "    - #### Standard Deviation & Variance\n",
    "    \n",
    "- ### Confidence Intervals\n",
    "     - #### Sampling Distribution: Probability distribution of a population statistic (mean), given a particular sample.\n",
    "     - #### Sample mean is the best, unbiased estimator of the population mean\n",
    "     - #### Confidence is dependant on sample variance & sampe size\n",
    "         - mean = sample mean\n",
    "         - varaince ~ sample variance / n (n=# of observations in the sample)\n",
    "         - std.dev ~ sample stdv / sqrt(n)\n",
    "\n",
    "- ### Skewness & Kurtosis\n",
    "    - #### Skewnesss looks at distortions due to caps and floors in the data\n",
    "    - #### Kurtosis is a measure of tail risk on a normal distribution, or likelyhood of extream events.\n",
    "    \n",
    "\n",
    " \n",
    "### Bivariate\n",
    "\n",
    "- ### Correlation & Covariance\n",
    "    - Correlation is a normalized version of covariance (-1,1)\n",
    "\n",
    "\n",
    "### Multivariate\n",
    "\n",
    "- ### Correlation Matrix & Covariance Matrix\n",
    "\n",
    "\n",
    "## Inferencial Statistics\n",
    "\n",
    "### Explain the elements of the data in terms of relationships with other elements\n",
    "\n",
    "- ### Hypothesis Testing - Answer the question is your story supported by the data?\n",
    "- ### Model Fitting - predictive analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-creator",
   "metadata": {},
   "source": [
    "# Numeric Feature Scaling\n",
    "\n",
    "### Mean Centering\n",
    "\n",
    "If you standardize your variables they have a mean of zero(Centering or Centered).\n",
    "\n",
    "Changing your original variables to a similar scale can help better interpret the resulting regression coefficients. Moreover, scaling is essential when using gradient descent-based algorithms because it facilitates quicker converging to a solution.\n",
    "\n",
    "Properly rescaling your predictors can help to fix missing values, allow advanced optimization techniques such as gradient descent, regularization, and stochastic learning, and easily detect outlying and anomalous values.\n",
    "\n",
    "The **StandardScaler** class (sklearn.preprocessing) will rescale your variables by removing the mean, an action also called centering. In fact, in your training set the rescaled variables will all have zero mean and the features will be forced to the unit variance. After fitting, the class will contain the mean_ and std_ vectors, granting you access to the means and standard deviations that made the scaling possible. Therefore, in any following set for testing purpose or predictions in production, you will be able to apply the exact same transformations, thus maintaining the data consistency necessary for the algorithm to work exactly.\n",
    "\n",
    "The **MinMaxScaler** class (sklearn.preprocessing) will rescale your variables setting a new minimum and maximum value in the range pointed out by you. After fitting, min_ and scale_ will report the minimum values (subtracted from the original variables) and the scale used for dividing your variables to have the intended maximum values, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#centering = StandardScaler(with_mean=True, with_std=False)\n",
    "#linear_regression.fit(centering.fit_transform(X),y)\n",
    "#print (\"coefficients: %s\\nintercept: %s\" % \\(linear_regression.coef_,linear_regression.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-blade",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Each standardized coefficient represents the unit change in the target after a modification in the predictors' equivalent to a standard deviation. However, the scales are not fully comparable if the distributions of our predictors are not normal (standardization implies a normal bell-shaped distribution), yet we can now compare the impact of each predictor and allow both the automatic handling of missing values and the correct functioning of advanced algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardization = StandardScaler(with_mean=True, with_std=True)\n",
    "#linear_regression.fit(standardization.fit_transform(X),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-postage",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization rescales as standardization, by acting on ranges of the predictors, but it has different properties. In fact, when using normalization the zero value is the minimum value in the range of values of each predictor. That means that zero doesn't represent the mean anymore. Moreover, rescaling between the maximum and the minimum can become misleading if there are anomalous values at either one of the extremities (most of your values will get squeezed around a certain region in [0,1], usually in the center of the range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling  = MinMaxScaler(feature_range=(0, 1))\n",
    "# linear_regression.fit(scaling.fit_transform(X),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-optimum",
   "metadata": {},
   "source": [
    "Applying the MinMaxScaler in a range of 0 and 1 drastically changes both the coefficients and the intercept, but this could be acceptable under certain circumstances. In fact, when working with big data derived from textual data or logs, we sometimes realize that the matrices we are working on are not especially populated with values, zero often being the most frequent value to be encountered. To speed up the calculations and allow huge matrices to be kept in memory, matrices are stored in a sparse format.\n",
    "\n",
    "Sparse matrices do not occupy all the memory necessary for their size, they just store coordinates and non-zero values. In such situations, standardizing the variables would change zero to the mean and a large number of previously zero cells would have to be defined, causing the matrix to occupy much more memory. Scaling between zero and one allows taking values in a comparable order and keeping all previously zero entries, thus not modifying the matrix dimensions in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-individual",
   "metadata": {},
   "source": [
    "## Qualitative Feature Encoding\n",
    "\n",
    "Transformed qualitative variables are called indicator or binary variables in machine learning terminology, whereas in statistics they are described as dichotomies (a more technical term) or **dummies variables**. They act in a regression formula as modifiers of the intercept when the level is present.\n",
    "\n",
    "When the levels of a variable are ordered the labels can also be converted into growing or decreasing numbers following the ordering of the meaning of the labels. Therefore, for example, good could be 3, average 2, acceptable 1, and bad 0. Such encoding directly translates a qualitative variable into a numeric one, but it works **only with labels that can be ordered** (that is, where you can define greater than and lower than relations). \n",
    "\n",
    "The transformation implies, since a single coefficient will be calculated in the regression model for all the levels, that the difference in the outcome passing from good to average is the same as passing from acceptable to bad. In reality, this is often not true due to non-linearity. In such a case, binary encoding is still the best solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_dummies(df)\n",
    "# DictVectorizer (sklearn)\n",
    "# CountVectorizer (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-electricity",
   "metadata": {},
   "source": [
    "## Numeric Feature Transformation\n",
    "\n",
    "Numeric features can be transformed, regardless of the target variable. This is often a prerequisite for better performance of certain classifiers, particularly distance-based. We usually avoid (besides specific cases such as when modeling a percentage or distributions with long queues) transforming the target, since we will make any pre-existent linear relationship between the target and other features non-linear.\n",
    "\n",
    "### Residual Analysis\n",
    "\n",
    "Pending\n",
    "\n",
    "### Binning\n",
    "\n",
    "When it is not easy to figure out the exact transformation, a quick solution could be to transform the continuous numeric variable into a series of binary variables, thus allowing the estimation of a coefficient for every single part of the numeric range of the variable.\n",
    "\n",
    "Though fast and convenient, this solution will increase the size of your dataset (unless you use a sparse representation of the matrix) and it will risk too much overfitting on your data.\n",
    "\n",
    "First, you divide your values into equally spaced bins and you notice the edges of the bins using the **histogram** function from Numpy. After that, using the **digitize** function, you convert the value in their bin number, based on the bin boundaries provided before. Finally, you can transform all the bin numbers into binary variables using the **LabelBinarizer** from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-template",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "emotional-branch",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-playlist",
   "metadata": {},
   "source": [
    "Passive Options\n",
    "- standardizing all the numeric variables\n",
    "- for as indicator variables, in order to passively intercept missing values, a possible strategy is instead to encode the presence of the label as 1 and its absence as -1, leaving the zero value for missing values.\n",
    "\n",
    "Explicit Options\n",
    "- np.nan_to_num\n",
    "- replace nan values with the series mean\n",
    "- Imputer (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-district",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "After properly transforming all the quantitative and qualitative variables and fixing any missing data, what's left is just to detect any possible outlier and to deal with it by removing it from the data or by imputing it as if it were a missing case.\n",
    "\n",
    "In order to detect outliers, there are a few approaches, some based on the observation of variables taken singularly (the single-variable, or univariate, approach), and some based on reworking all the variables together into a synthetic measure (the multivariate approach).\n",
    "\n",
    "The best single variable approach is based on the observation of standardized variables and on the plotting of box plots:\n",
    " - Using standardized variables, everything scoring further than the absolute value of three standard deviations from the mean is suspect, though such a rule of thumb doesn't generalize well if the distribution is not normal\n",
    " - Using boxplots, the interquartile range (shortened to IQR; it is the difference between the values at the 75th and the 25th percentile) is used to detect suspect outliers beyond the 75th and 25th percentiles. If there are examples whose values are outside the IQR, they can be considered suspicious, especially if their value is beyond 1.5 times the IQR's boundary value. If they exceed 3 times the IQR's limit, they are almost certainly outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-assurance",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "Ravi, Janani. Interpreting Data Using Descriptive Statistics with Python, 8 Nov 2019, pluralsight.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-harmony",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
